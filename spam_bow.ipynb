{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpus and drop the missing rows\n",
    "df = pd.read_csv(\"~/Desktop/email_corpus.csv\")\n",
    "df = df.dropna()\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>file</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>00249.5f45607c1bffe89f60ba1ec9f878039a</td>\n",
       "      <td>zzzzasonorg mailsweeperabc-arbitragecom mailsw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>00373.ebe8670ac56b04125c25100a36ab0510</td>\n",
       "      <td>zzzzasonorg webnotenet serverhub webnotenet mx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>00214.1367039e50dc6b7adb0f2aa8aba83216</td>\n",
       "      <td>zzzzasonorg mail pickup service microsoft smtp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>00210.050ffd105bd4e006771ee63cabc59978</td>\n",
       "      <td>zzzzasonorg x-authentication-warning didn't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>00033.9babb58d9298daa2963d4f514193d7d6</td>\n",
       "      <td>phoboslabsspamassassintaintorg mailwebnotenet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0</td>\n",
       "      <td>01037.6b42b5f3d3d9e6293bf24af66b250655</td>\n",
       "      <td>phoboslabsnetnoteinccom phobos egwnnet egwnnet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0</td>\n",
       "      <td>02056.7bc7703e40a24dda665d4ce7b0cba710</td>\n",
       "      <td>gamasutra ryder cup url europe united states e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0</td>\n",
       "      <td>01782.278f53b8f65fcd422cb26c5bbe74599d</td>\n",
       "      <td>[use perl] stories precedence list x-bulkmail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0</td>\n",
       "      <td>00043.d2673a72d215cbdd747dc98cde41fbd2</td>\n",
       "      <td>phoboslabsnetnoteinccom phobos lughtuathaorg l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>0</td>\n",
       "      <td>00135.bd3bc1c036eab89c9c50cff40958c939</td>\n",
       "      <td>spamassassintaintorg x-egroups-return nnfmp x-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam                                    file  \\\n",
       "id                                                   \n",
       "0        1  00249.5f45607c1bffe89f60ba1ec9f878039a   \n",
       "1        1  00373.ebe8670ac56b04125c25100a36ab0510   \n",
       "2        1  00214.1367039e50dc6b7adb0f2aa8aba83216   \n",
       "3        1  00210.050ffd105bd4e006771ee63cabc59978   \n",
       "4        1  00033.9babb58d9298daa2963d4f514193d7d6   \n",
       "...    ...                                     ...   \n",
       "2997     0  01037.6b42b5f3d3d9e6293bf24af66b250655   \n",
       "2998     0  02056.7bc7703e40a24dda665d4ce7b0cba710   \n",
       "2999     0  01782.278f53b8f65fcd422cb26c5bbe74599d   \n",
       "3000     0  00043.d2673a72d215cbdd747dc98cde41fbd2   \n",
       "3001     0  00135.bd3bc1c036eab89c9c50cff40958c939   \n",
       "\n",
       "                                                  words  \n",
       "id                                                       \n",
       "0     zzzzasonorg mailsweeperabc-arbitragecom mailsw...  \n",
       "1     zzzzasonorg webnotenet serverhub webnotenet mx...  \n",
       "2     zzzzasonorg mail pickup service microsoft smtp...  \n",
       "3     zzzzasonorg x-authentication-warning didn't us...  \n",
       "4     phoboslabsspamassassintaintorg mailwebnotenet ...  \n",
       "...                                                 ...  \n",
       "2997  phoboslabsnetnoteinccom phobos egwnnet egwnnet...  \n",
       "2998  gamasutra ryder cup url europe united states e...  \n",
       "2999  [use perl] stories precedence list x-bulkmail ...  \n",
       "3000  phoboslabsnetnoteinccom phobos lughtuathaorg l...  \n",
       "3001  spamassassintaintorg x-egroups-return nnfmp x-...  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = df['words'].values\n",
    "Ys = df['spam'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs,\n",
    "                                                    Ys,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-61d2a3d76671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of different words: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Word example: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5369\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check the split printing the shape of each set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \"\"\"\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         return [t for t, i in sorted(self.vocabulary_.items(),\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Number of different words: {0}\".format(len(feature_names)))\n",
    "print(\"Word example: {0}\".format(feature_names[5369]))\n",
    "\n",
    "# Check the split printing the shape of each set.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4fb3f9cda3c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                        stop_words='english')\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1869\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \"\"\"\n\u001b[0;32m-> 1871\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m         \u001b[0;31m# FIXME Remove copy parameter support in 0.24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "email = [\"Hello George, how about a game of tennis tomorrow?\",\n",
    "         \"Hello, click here if you want to satisfy your wife tonight\",\n",
    "         \"We offer free viagra!!! Click here now!!!\",\n",
    "         \"Dear Sara, I prepared the annual report. Please check the attachment.\",\n",
    "         \"Hi David, will we go for cinema tonight?\",\n",
    "         \"Best holidays offers only here!!!\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                       stop_words='english')\n",
    "\n",
    "examples = vectorizer.transform(email)\n",
    "predictions = clf.predict(examples)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the sample into 80% for training and 20% for testing\n",
    "df['mask'] = [np.random.uniform(0,1)  for k in df.index]\n",
    "train = df[df['mask'] < 0.8]\n",
    "test = df[df['mask']>= 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists of words that bring together all of the words found\n",
    "# in the spam and ham training rows\n",
    "spamwords = []\n",
    "spamcount = 0\n",
    "hamwords = []\n",
    "hamcount = 0\n",
    "for index, row in train.iterrows():\n",
    "    li = list(row['words'].split(' '))\n",
    "    if row['spam']==1:\n",
    "        spamcount = spamcount + 1\n",
    "        spamwords = spamwords + li\n",
    "    else:\n",
    "        hamcount = hamcount + 1\n",
    "        hamwords = hamwords + li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in all of the ham emails:  27921\n"
     ]
    }
   ],
   "source": [
    "# getting all of the words found in the ham emails\n",
    "hamdist = nltk.FreqDist(hamwords)\n",
    "hamset = []\n",
    "for each in hamdist:\n",
    "    hamset.append(each)\n",
    "print(\"The number of words in all of the ham emails: \", len(hamset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in all of the spam emails:  10852\n"
     ]
    }
   ],
   "source": [
    "# getting all of the words found in the spam emails\n",
    "spamdist = nltk.FreqDist(spamwords)\n",
    "spamset = []\n",
    "for each in spamdist:\n",
    "    spamset.append(each)\n",
    "print(\"The number of words in all of the spam emails: \",len(spamset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a Venn Diagram perspective, we create two functions to help us\n",
    "# understand the set of words\n",
    "\n",
    "#this function returns all of the words shared between two sets\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "# this function returns all of the words found only in the first set\n",
    "def difference (lst1, lst2):\n",
    "    result = []\n",
    "    for element in lst1:\n",
    "        if element not in lst2:\n",
    "            result.append(element)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words shared by ham and spam:  5761\n",
      "The number of words only in spam:  5091\n",
      "The number of words only in ham:  22160\n"
     ]
    }
   ],
   "source": [
    "# we now look at taking the two sets and making three sets\n",
    "# intersection_set:  words found in both ham and spam\n",
    "# spamonly_set:  words found in spam but not ham\n",
    "# hamonly_set:  words found in ham but not spam\n",
    "    \n",
    "intersection_set = intersection(hamset, spamset)\n",
    "print(\"The number of words shared by ham and spam: \",len(intersection_set))\n",
    "\n",
    "spamonly_set = difference (spamset, hamset)\n",
    "print(\"The number of words only in spam: \", len(spamonly_set))\n",
    "\n",
    "hamonly_set = difference (hamset, spamset)\n",
    "print(\"The number of words only in ham: \", len(hamonly_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go through the full dataframe now and get a count of ham, spam,\n",
    "# and shared words\n",
    "\n",
    "# create a column in the dataframe for spamcount\n",
    "df['spamcount']=0\n",
    "df['hamcount']=0\n",
    "df['sharedcount']=0\n",
    "\n",
    "for ind in df.index:\n",
    "    li = list(df.loc[ind, 'words'].split(' '))\n",
    "    df.loc[ind, 'spamcount'] = len(intersection(li, spamonly_set))\n",
    "    df.loc[ind, 'hamcount'] = len(intersection(li, hamonly_set))\n",
    "    df.loc[ind, 'sharedcount'] = len(intersection(li, intersection_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[495   2]\n",
      " [  7 101]]\n"
     ]
    }
   ],
   "source": [
    "# one approach we can take is to look at hamcount and spamcount\n",
    "# and just see which one is larger.  If spamcount>=hamcount,\n",
    "# then it seem logical that it is a spam email.  However,\n",
    "# when spamcount<hamcount, then it probably is ham.  Let's\n",
    "# try this on our testing data set.\n",
    "\n",
    "df['simple_prediction']=0 #default is that email is ham unless the condition is met\n",
    "df.loc[df['spamcount']>=df['hamcount'], 'simple_prediction']= 1\n",
    "\n",
    "df['result']='unknown'\n",
    "df.loc[(df['simple_prediction']==1) & (df['spam']==1), 'result']= \"true positive\"\n",
    "df.loc[(df['simple_prediction']==0) & (df['spam']==1), 'result']= \"false negative\"\n",
    "df.loc[(df['simple_prediction']==1) & (df['spam']==0), 'result']= \"false positive\"\n",
    "df.loc[(df['simple_prediction']==0) & (df['spam']==0), 'result']= \"true negative\"\n",
    "\n",
    "train = df[df['mask'] < 0.8]\n",
    "test = df[df['mask']>= 0.8]\n",
    "results = confusion_matrix(test['spam'], test['simple_prediction'])\n",
    "print ('Confusion Matrix :')\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  2 :  ['zzzzasonorg', 'hallo']\n",
      "HAM  3 :  ['english', 'bother', 'russian']\n",
      "\n",
      "\n",
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  2 :  ['zzzzasonorg', 'adv']\n",
      "HAM  4 :  ['enterprises', 'ass', 'x-accept-language', 'beach']\n",
      "\n",
      "\n",
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  9 :  ['purchasing', 'traditionally', 'mlm', \"'remove'\", 'multi-level', 'intrusion', 'zzzzasonorg', 'risk-free', 'miracle']\n",
      "HAM  24 :  ['leg', 'carbon', '\"look', 'exception', 'ingenious', 'scream', 'winner', 'smiling', 'flaws', 'cautious', 'hoopla', 'likes', 'bones', 'overly', 'deep', 'definitely', 'awhile', 'positions', 'initially', 'arm', 'convinced', 'excited', 'magic', 'dubious']\n",
      "\n",
      "\n",
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  4 :  ['attained', 'zzzzasonorg', 'cocks', 'overlook']\n",
      "HAM  6 :  ['sucking', 'scared', 'gauge', 'showed', 'hall', 'tons']\n",
      "\n",
      "\n",
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  14 :  ['helvetica', 'llc\"', 'sans-serif', 'warehouse', 'padding-right', 'arial', 'padding-left', 'none\"', 'lbs', 'font-size', 'aluminum', 'liabilities', 'zzzzasonorg', 'background-color']\n",
      "HAM  45 :  ['smallest', 'emit', 'martin', 'cat', 'prove', 'distributor', 'porous', 'inset', 'cushions', 'lecture', 'particularly', 'melt', 'barrier', 'cups', 'william', 'rubber', 'seat', 'covered', 'chapter', 'electricity', 'spray', 'otto', 'quietly', 'obsolete', 'ignoring', 'perform', 'pda', 'blaming', 'gas', 'measuring', 'definitely', 'resin', '\"industry', 'bogus', 'welcomes', 'blends', 'australian', 'contribute', 'standardized', 'convection', 'tank', 'stupid', 'garbage', 'spaces', 'self-contained']\n",
      "\n",
      "\n",
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  13 :  ['furnish', 'seizure', 'indulgence', 'utmost', 'screening', 'intl', 'modalities', 'crave', 'bodyguards', 'nos', 'zzzzasonorg', 'implore', 'kabila']\n",
      "HAM  20 :  ['modesty', \"everyone's\", 'suddenly', 'pray', 'await', 'threatened', 'extraordinarily', 'col', 'stems', 'tribe', 'personnel', 'gathered', 'aides', 'convinced', 'bloody', 'hostile', 'assisting', 'soul', 'privately', 'careful']\n",
      "\n",
      "\n",
      "Is it spam?:  1 \n",
      " false negative\n",
      "SPAM  4 :  ['x-status', 'coltd', 'x-keywords', 'leramilerctrorg']\n",
      "HAM  5 :  ['nights', 'stepped', 'buildings', 'sen', 'projection']\n",
      "\n",
      "\n",
      "Is it spam?:  0 \n",
      " false positive\n",
      "SPAM  2 :  ['jam', 'defective']\n",
      "HAM  2 :  ['zawodny', 'fond']\n",
      "\n",
      "\n",
      "Is it spam?:  0 \n",
      " false positive\n",
      "SPAM  2 :  ['qualifying', 'slovakia']\n",
      "HAM  2 :  ['guardian', 'recalled']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's do a deep dive into the errors and see if we can find words that could be\n",
    "# added to the intersection_set\n",
    "df_errors = df[(df['result']=='false positive') | (df['result']=='false negative')]\n",
    "for index, row in df_errors.iterrows():\n",
    "    li = list(row['words'].split(' '))\n",
    "    print('Is it spam?: ', row['spam'], \"\\n\", row['result'])\n",
    "    print(\"SPAM \", row['spamcount'], \": \", intersection(li, spamonly_set))\n",
    "    print(\"HAM \", row['hamcount'], \": \", intersection(li, hamonly_set))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the analysis above, we can create a set of words\n",
    "# we want to move from the spamonly_set into the intersection_set\n",
    "\n",
    "move_list = [] # put words in this list to be moved\n",
    "for each in move_list:\n",
    "    spamonly_set.remove(each)\n",
    "    intersection_set.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  101\n",
      "True negatives:  495\n",
      "False positives:  2\n",
      "False negatives:  7\n"
     ]
    }
   ],
   "source": [
    "# redoing the analysis with the updated word sets\n",
    "for ind in df.index:\n",
    "    li = list(df.loc[ind, 'words'].split(' '))\n",
    "    df.loc[ind, 'spamcount'] = len(intersection(li, spamonly_set))\n",
    "    df.loc[ind, 'hamcount'] = len(intersection(li, hamonly_set))\n",
    "    df.loc[ind, 'sharedcount'] = len(intersection(li, intersection_set))\n",
    "\n",
    "df['simple_prediction']=0 #default is that email is ham unless the condition is met\n",
    "df.loc[df['spamcount']>=df['hamcount'], 'simple_prediction']= 1\n",
    "df['result']='unknown'\n",
    "df.loc[(df['simple_prediction']==1) & (df['spam']==1), 'result']= \"true positive\"\n",
    "df.loc[(df['simple_prediction']==0) & (df['spam']==1), 'result']= \"false negative\"\n",
    "df.loc[(df['simple_prediction']==1) & (df['spam']==0), 'result']= \"false positive\"\n",
    "df.loc[(df['simple_prediction']==0) & (df['spam']==0), 'result']= \"true negative\"\n",
    "\n",
    "train = df[df['mask'] < 0.8]\n",
    "test = df[df['mask']>= 0.8]\n",
    "\n",
    "print(\"True positives: \", test[test['result'] == \"true positive\"].count()[\"result\"])\n",
    "print(\"True negatives: \", test[test['result'] == \"true negative\"].count()[\"result\"])\n",
    "print(\"False positives: \", test[test['result'] == \"false positive\"].count()[\"result\"])\n",
    "print(\"False negatives: \", test[test['result'] == \"false negative\"].count()[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
