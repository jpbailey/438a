{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are interested in building a model and predicting\n",
    "# who survived for a holdout sample, please take a look at:\n",
    "# https://www.kaggle.com/c/titanic/overview\n",
    "\n",
    "df = pd.read_csv('~/Desktop/titanic_train.csv')\n",
    "df = df.set_index('PassengerId')\n",
    "\n",
    "dummies = pd.get_dummies(df['Sex'], drop_first=True)\n",
    "df = pd.concat([df, dummies], axis = 1)\n",
    "dummies = pd.get_dummies(df['Embarked'], drop_first=True)\n",
    "df = pd.concat([df, dummies], axis = 1)\n",
    "df = df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a new column in the dataframe that creates\n",
    "# two clusters - died and survived\n",
    "\n",
    "def Assess(row):\n",
    "     if row==1:\n",
    "        return 'survived'\n",
    "     return 'died'\n",
    "\n",
    "df['outcome'] = df.apply(lambda x: Assess(x.Survived), axis=1)\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test dataframes based on 80% train and 20% test\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.2, stratify = df['outcome'], random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating out the independent and dependent variable labels\n",
    "\n",
    "fn = ['Pclass', 'Age', 'Fare','male']\n",
    "cn = ['died', 'survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the independent and dependent variables for two\n",
    "# models - logistic regression and classification\n",
    "\n",
    "X_train = train[fn]\n",
    "Yvar_train = train.Survived\n",
    "y_train = train.outcome\n",
    "X_test = test[fn]\n",
    "Yvar_test = test.Survived\n",
    "y_test = test.outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression - we have seen this before\n",
    "\n",
    "LogisticModel = sm.Logit(Yvar_train, X_train).fit()\n",
    "print(LogisticModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now use this model to help examine our testing set\n",
    "\n",
    "YPred = LogisticModel.predict(X_test)\n",
    "compare = pd.concat([Yvar_test, YPred], axis=1)\n",
    "compare = compare.rename(columns={0: 'prediction'})\n",
    "compare['difference'] = compare['Survived'] - compare['prediction']\n",
    "compare.sort_values(by=['difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through the residuals to generate a confusion matrix\n",
    "\n",
    "def rating(difference):\n",
    "    if difference > 0.5:\n",
    "        return \"false negative\"\n",
    "    elif difference >= 0:\n",
    "        return \"true positive\"\n",
    "    elif difference >= -0.5:\n",
    "        return \"true negative\"\n",
    "    else:\n",
    "        return \"false positive\"\n",
    "\n",
    "compare['type'] = compare.apply(lambda x: rating(x['difference']),axis=1)\n",
    "true_positives = compare[compare['type'] == \"true positive\"].count()[\"type\"]\n",
    "true_negatives = compare[compare['type'] == \"true negative\"].count()[\"type\"]\n",
    "false_positives = compare[compare['type'] == \"false positive\"].count()[\"type\"]\n",
    "false_negatives = compare[compare['type'] == \"false negative\"].count()[\"type\"]\n",
    "print(\"True positives: \", true_positives)\n",
    "print(\"True negatives: \", true_negatives)\n",
    "print(\"False positives: \", false_positives)\n",
    "print(\"False negatives: \", false_negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we haven't seen this before, but we are going to measure\n",
    "# the Receiver Operating Characteristic (ROC)\n",
    "# https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "roc_auc_score(Yvar_test, YPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then take a look at the ROC curve\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(Yvar_test, YPred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "# plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we try a different approach with a classification method using\n",
    "# decision trees\n",
    "\n",
    "mod_dt = DecisionTreeClassifier(max_depth = 2, random_state = 1)\n",
    "mod_dt.fit(X_train,y_train)\n",
    "prediction=mod_dt.predict(X_test)\n",
    "print(\"The accuracy of the Decision Tree is\",\"{:.3f}\".format(metrics.accuracy_score(prediction,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plot_tree(mod_dt, feature_names = fn, class_names = cn, filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
